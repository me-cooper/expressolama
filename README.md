# expressolama
Simple Node.JS Server with frontend to interact with oolama server.




## Requirements

Run local ollama server:
https://makesmart.net/threads/read/mit-ollama-und-drei-befehlen-zum-lokalen-llm-auf-dem-mac

Use Node ollama Wrapper class:
https://github.com/me-cooper/ollamanode


## Start Server


```shell
node server.js
```


And chat with local ai